{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1c8a120e8d43a9849f928c109f4d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e2090337eb40feb6b668f292b5de95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'AvgAvgAvgSalesReasonIDIsAppropriateCILowerBound'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AvgAvgAvgSalesReasonIDIsAppropriateCILowerBound'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustomer_analyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomerAnalyzer\n\u001b[1;32m----> 2\u001b[0m ca\u001b[38;5;241m=\u001b[39m\u001b[43mCustomerAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmssql+pyodbc://localhost/AdventureWorks2022?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\DataScience\\03_sql\\kunskapskontroll_sql\\customer_analyzer.py:206\u001b[0m, in \u001b[0;36mCustomerAnalyzer.__init__\u001b[1;34m(self, connection_string)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,connection_string: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_dfs__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\DataScience\\03_sql\\kunskapskontroll_sql\\customer_analyzer.py:223\u001b[0m, in \u001b[0;36mCustomerAnalyzer.__get_dfs__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__demordsreason_merge_orddet__()\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__create_data_df_and_detailed_data_df__()\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__create_aggregated_data_df__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\DataScience\\03_sql\\kunskapskontroll_sql\\customer_analyzer.py:419\u001b[0m, in \u001b[0;36mCustomerAnalyzer.__create_aggregated_data_df__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Filter\u001b[39;00m\n\u001b[0;32m    418\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentNumDistinctCustomers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m]\n\u001b[1;32m--> 419\u001b[0m df2\u001b[38;5;241m=\u001b[39mdf2[\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAvgAvgAvgSalesReasonIDIsAppropriateCILowerBound\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m]\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Sort and reset index\u001b[39;00m\n\u001b[0;32m    422\u001b[0m df2\u001b[38;5;241m.\u001b[39msort_values(\n\u001b[0;32m    423\u001b[0m     by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgAvgAvgAdjustedProfitCILowerBound\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    424\u001b[0m     ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m     inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    426\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AvgAvgAvgSalesReasonIDIsAppropriateCILowerBound'"
     ]
    }
   ],
   "source": [
    "from customer_analyzer import CustomerAnalyzer\n",
    "ca=CustomerAnalyzer(f\"mssql+pyodbc://localhost/AdventureWorks2022?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fictitious scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher-ups at Adventureworks have a firm beleif in nurturing existing customer relations and in creating a steady flow of new customers. This philosophy applies not only to the big store customers but to private individual customers as well. \n",
    "\n",
    "As a consequence, a decision has been made to target 500 existing most valued private individual US-customers and give them a no strings attached, one-time, one-product, but any product 40 % discount. The same is to be done for 500 individuals in the general US-population. At lower levels in the organization, especially among the business analysts, this decision has sparked a discussion about whether it is at all possible to implement this decision in such a way that we can expect to actually make money on the sales with these discounted prices. \n",
    "\n",
    "A junior analyst decides to answer this question using a data driven approach where a market segmentation, based on available demographic data, is performed and an investigation of whether there are market segments, where Adventureworks can expect to be profitable even at these highly discounted prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this analaysis is to find at least one segment of the US, private individual market that is profitable even at an across the board 40% discount rate. To this end, we segment this market based on available demographic data in the existing customer data base. These demographics are as follows:\n",
    "\n",
    "1. Age\n",
    "2. Marital Status (Single/Married)\n",
    "3. Education Level\n",
    "4. Income Level\n",
    "5. Gender\n",
    "6. Whether the person is a home owner\n",
    "7. Whether the person is a car owner\n",
    "8. Whether the peson has children in the household\n",
    "\n",
    "We then proceed to calculate two quantities:\n",
    "\n",
    "1. A profitability score for each segment, the unit of which is USD\n",
    "2. A price and promotion sensitivity score between 0 and 1 for each segment.\n",
    "\n",
    "The exact definitions of these measures are found in the detailed analysis. Since the explicitly stated purpose of the discount is to show customer appreciation and to attract new customers, we require the second quantity to be rather high, at least 0.75 at the 95% confidence level, so that we can be relatively confident that those targeted will have good use for the discount. \n",
    "\n",
    "It turns out that with these restrictions applied, there is exactly one segment, consisting of 508 customers, that has a positive profatibility score at the 95% confidence level: Above age 50, married, high education, high income men who own a home and a car, and who have children in the household. We recommend that this segment should be targeted for the discount. Below are the top five segments, ordered by their profitability score lower bound, with the discount taken into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_top_five_segments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the top 5 selling products in the targeted segment and their respective profitability score lower bounds and sales volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_top_five_products('01000111')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we refrain from doing so here, further profit optimization could be made by only targeting specific products for the discount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by performing the segmentation. Using the Sales.Customers and Person.Person tables, we may construct the following dataframe of binary demographics data for US private individual customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {ca.demogr_df.shape[0]}')\n",
    "ca.demogr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need some form of measure of responsiveness to price and promotion within each market segment. Using the tables Sales.SalesOrderHeader and Sales.Sales.OrderHeaderSalesReason, we construct a binary variable SalesReasonIDIsAppropriate which takes the value 1 if one of the reasons for the sale was \"Price\" or \"Promotion\" and 0 otherwise. We will later use this variable to define the price and promotion sensitivity measure. The updated dataframe looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {ca.demordsreason_df.shape[0]}')\n",
    "ca.demordsreason_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that the number of rows has increased, which is natural since one customer can have many orders. We will also need a measure of profitability within each segment. To construct such a measure, we need detailed data about the contents of the order such as order quantities, product ids and prices. This information can be found in the Sales.SalesOrderDetail table. The updated dataframe has the following appearance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {ca.demordsreasonorddet_df.shape[0]}')\n",
    "ca.demordsreasonorddet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to investigate the contents of the OrderQty and UnitPrice columns since these will affect how we calculate the profits. The minimum and maximum values of these are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'OrderQty minimum: {ca.demordsreasonorddet_df.OrderQty.min()}')\n",
    "print(f'OrderQty maximum: {ca.demordsreasonorddet_df.OrderQty.min()}')\n",
    "print(f'UnitPriceDiscount minimum: {ca.demordsreasonorddet_df.UnitPriceDiscount.min()}')\n",
    "print(f'UnitPriceDiscount maximum: {ca.demordsreasonorddet_df.UnitPriceDiscount.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that these columns both have constant values and so we don't need to take them into account. We need one more piece of data to be able to perform the analysis: The cost of each product. This information is available in the Production.Product table via the StandardCost column. We add this column to our dataframe. Then we define the measure AdjustedProfit as 0.6 * LineTotal - StandardCost. This measure takes into account the 40% discount. We compress the demographics column into a single column using concatenation to form a segment label and remove columns we don't need. The final dataframe (sorted on labels) containing the data we need now looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {ca.data_df.shape[0]}')\n",
    "ca.data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the number of rows has not changed, which is as it should be. We also note that there are rows with a negative profit. These are rows where the product sold is such that we actually loose money by giving a 40% reduction in price. We want to examine if there are any segments, where we can expect to make money even with the 40% reduction in price. There is one important thing to note here: The dataset has a hierachical structure. One customer can have many orders and one order has many products in it. It is not overly paranoid to suspect that there are intra level dependencies. For instance, perhaps it is more likely that you buy a pump, given that you buy a tyre under one and the same order. It may be the case that customers have repeated orders containing the same products. As a consequence, we cannot consider the AdjustedProfit and SalesReasonIDIsAppropriate columns of our dataframe to be i.i.d samples at the lowest levels in the hierarchy and so the central limit theorem may not apply for the calcualation of condidence intervals. To destroy intra level dependencies and end up with (close to) i.i.d samples of measures at the customer level, we proceed as follows:\n",
    "\n",
    "1. Group at the order level and calculate the average adjusted profit and average SalesReasonIDIsAppropriate and produce a new dataframe at this level\n",
    "\n",
    "2. Group the new dataframe at the customer level and calculate \"Averages of averages\" and produce a new dataframe at this level\n",
    "\n",
    "3. The measures can now be calculated at the segment level as averages since the values at the customer level are (close to) i.i.d\n",
    "\n",
    "4. Calculate lower bounds for these two measures at the 95% confidence level. Since there has been a bit of trickery involved, we calculate lower bounds using both the central limit theorem and by bootstrapping. We then conservatively choose the lowest of these bounds to be the bound under consideration.\n",
    "\n",
    "We require there to be at least 50 customers in the segment to ensure central limit theorem applicability for the calculation of lower bounds. Also, we want our chosen customers to actually use the discount, because only then, can we be relatively certain that the \"softer\" intentions of the discount, such as the customer feeling appreciated, will be achieved. We therefore require the lower bound for price and promotion sensitivity to be at least 0.75. \n",
    "\n",
    "With these restrictions, the aggregated data is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.aggregated_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there are several segments for which the lower bound for the average adjusted profit is positive at the 95% confidence level. The segment with the highest lower bound is the segment with label '01000111'. This is the segment we should target. The description for this segment is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.label_to_description('01000111')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To those who have, more shall be given, it would seem. We note that there is at least one segment with a substantially higher point estimate for the average adjusted profit than the one we target. The lower bound, however, is not higher. This indicates that within that segment, the \"spread\" of average profit over different products is higher than within our targeted segment. What products then, should we expect to sell in our targeted \"alpha male\" segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=ca.get_top_products('01000111')\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that tires and tubes are the top sellers. We also note that among these top sellers there are products with a negative lower bound for the profit. These, however, are compensated for by the products with positive lower bound and we arrive at a positive estimated lower bound for the segment as a whole. This concludes the detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta analysis (caveats and considerations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found a market segment that is profitable, even at a discount rate of 40%. This market segment consists of men who have come of age and who are successful at life. They own high cost assets such as houses and cars. It does seem plausible that such a market segment can and will purchase products where we have a high enough profit margin that, even at a 40% discount rate, we are still profitable in the segment as a whole.  \n",
    "\n",
    "As with any statistical analysis however, there are considerations and caveats to be made. Here are a few:\n",
    "\n",
    "1. If the customers and patterns of purchase in our database is not a representative sample of the market, the conclusion made in this analysis may not hold true. We do however, have the odds on our side that this is not the case, since we are dealing with a large enough sample that the probability of producing a pathological sample of that size is low.\n",
    "\n",
    "2. We are calculating our point- and interval estimates at the line item level. Since there are more line items than there are orders and more orders than there are customers, it cannot be ruled out that our sample is not truly independent. There may be dependencies between products within the same order and between orders for the same customer. Although the central limit theorem is robust to slight violations of the assumptions involved, this is a cause for some concern and warrants further investigation.\n",
    "\n",
    "3. There is a possibility that the 40% discount will change the distribution of sales volumes over the product categories. Given a 40% discount, you may not go for \"Tires and tubes\", but rather for more expensive products such as bikes. We deem it unlikely that the distribution within the \"Bike\" category changes much though and since this distribution is evenly distributed over Road Bikes (-55.251087 lower bound for average profit) and Mountain bikes (94.726640 lower bound for average profit) we should be profitable, even if there is such an effect.\n",
    "\n",
    "As noted above, point 2 warrants further investigation. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
